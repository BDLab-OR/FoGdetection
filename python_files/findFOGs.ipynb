{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import resampy as rs \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa as tsa \n",
    "import statsmodels.formula.api as smf\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_directory = '/Users/alexastefanko/lab/FoGdetection/Subjects'\n",
    "output_path = '/Users/alexastefanko/lab/FoGdetection/Output/fogs.xlsx'\n",
    "sample_rate = 128\n",
    "step = 200\n",
    "\n",
    "r_acc_ap_col = \"R_acc\"; #Column name for right accelerometer anterior-posterior\n",
    "l_acc_ap_col = \"L_acc\"; #Column name for left accelerometer anterior-posterior\n",
    "r_gyr_ml_col = \"R_gyr\"; #Column name for right gyroscope mediolateral\n",
    "l_gyr_ml_col = \"L_gyr\"; #Column name for left gyroscop mediolateral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_files(directory):\n",
    "    for dir_name, subdir_list, file_list in os.walk(directory):\n",
    "        return [os.path.join(dir_name, name) for name in file_list if name.endswith(\".csv\")] \n",
    "        \n",
    "def get_starter_idx(data):\n",
    "    r_ml = data[0]\n",
    "    mean_r = np.mean(r_ml)\n",
    "    r_for_cov = [x - mean_r for x in r_ml]\n",
    "    \n",
    "    l_ml = data[1]\n",
    "    mean_l = np.mean(l_ml)\n",
    "    l_for_cov = [x - mean_l for x in l_ml]\n",
    "    \n",
    "    #R and L angular velocities cross-correlation to find a delay between two\n",
    "    cross_cov = np.correlate(r_for_cov, l_for_cov, \"full\")\n",
    "    shifter = 0\n",
    "    for idx, num in enumerate(cross_cov):\n",
    "        if num == np.max(cross_cov):\n",
    "            shifter = idx\n",
    "    starter = round(abs(shifter - (len(cross_cov)/2)))\n",
    "    return starter\n",
    "\n",
    "def create_corr_log(corr_data, threshold):\n",
    "    log = [None] * len(corr_data)\n",
    "    idx = 0\n",
    "    for i in corr_data:\n",
    "        if i < threshold:\n",
    "            log[idx] = 1\n",
    "        else:\n",
    "            log[idx] = 0\n",
    "        idx = idx + 1\n",
    "    return log\n",
    "    \n",
    "def correlation_based_method(starter, filtered_data):\n",
    "    #Sync R and L after correction for delay\n",
    "    t_sensor_1 = filtered_data[0][starter:]\n",
    "    t_sensor_2 = filtered_data[1][:len(filtered_data[1]) - starter]\n",
    "\n",
    "    correlated_sensor_data = []\n",
    "    \n",
    "    #Ensure bouts have sufficient data\n",
    "    range_intervals = list(range(0, len(t_sensor_1) - (step+1), step))\n",
    "    if len(t_sensor_1) > (step*5):\n",
    "        for i in range(len(range_intervals) - 1):\n",
    "            start = range_intervals[i]\n",
    "            end = range_intervals[i+1]\n",
    "            corr = np.corrcoef(t_sensor_1[start:end], t_sensor_2[start:end])\n",
    "            correlated_sensor_data.append(corr[0][1])\n",
    "        abs_corr_sensor_data = np.absolute(correlated_sensor_data)\n",
    "        corr_log = create_corr_log(abs_corr_sensor_data, 0.5)\n",
    "        return corr_log\n",
    "    else:\n",
    "        raise ValueError(\"Error: There is not enough data to calculate FOG. Please ensure each bout is longer than 10 seconds or sample more often\")\n",
    "\n",
    "def fft_method(starter, data, len_data):\n",
    "    corrected_r_ap = data[0][starter:]\n",
    "    corrected_l_ap = data[1][:len_data - starter]\n",
    "    if len(corrected_r_ap) > step*5:\n",
    "        range_intervals = list(range(0, len(corrected_r_ap) - (step+1), step))\n",
    "        ratio_r = []\n",
    "        ratio_l = []\n",
    "        for i in range(len(range_intervals) - 1):\n",
    "            start = range_intervals[i]\n",
    "            end = range_intervals[i+1]\n",
    "            l = len(corrected_r_ap[start:end])\n",
    "            list_step = int(round(l/step))\n",
    "            f = list(range(0, (list_step*(l)), list_step))\n",
    "            lf = f.index(3)\n",
    "            hf = f.index(10)\n",
    "            llf = f.index(0)\n",
    "            detrend_r = sp.signal.detrend(corrected_r_ap[start:end])\n",
    "            detrend_l = sp.signal.detrend(corrected_l_ap[start:end])\n",
    "            p_r = abs(np.fft.fft(detrend_r))/(l/2)\n",
    "            p_l = abs(np.fft.fft(detrend_l))/(l/2)\n",
    "            ratio_r.append(sum(p_r[lf:hf+1])**2/sum(p_r[llf:lf+1])**2)\n",
    "            ratio_l.append(sum(p_l[lf:hf+1])**2/sum(p_l[llf:lf+1])**2)\n",
    "        perc = [None] * len(ratio_r)\n",
    "        for j in range(len(ratio_r)):\n",
    "            # Threshold on frequency ratio based on FFT\n",
    "            if ratio_r[j]>10 or ratio_l[j] > 10:\n",
    "                perc[j] = 1\n",
    "            else:\n",
    "                perc[j] = 0\n",
    "    else:\n",
    "        raise ValueError(\"Error: There is not enough data to calculate FOG. Please ensure each bout is longer than 10 seconds or sample more often\")\n",
    "    return perc\n",
    "\n",
    "def find_log_overlap(corr_log, fft_log):\n",
    "    final_log = [None] * len(corr_log)\n",
    "    for i in range(len(corr_log)):\n",
    "        if corr_log[i] == 1 and fft_log[i] == 1:\n",
    "            final_log[i] = 1\n",
    "        else:\n",
    "            final_log[i] = 0\n",
    "    return final_log\n",
    "\n",
    "def merge_one_two_second(log):\n",
    "    #this log should be its own data struct\n",
    "    merged_log = log \n",
    "    episodes = [i for i, x in enumerate(log) if x == 1]\n",
    "    episodes_diff = np.diff(episodes)\n",
    "    indices_diff_1 = [i for i, x in enumerate(episodes_diff) if x == 2]\n",
    "    indices_diff_2 = [i for i, x in enumerate(episodes_diff) if x == 3]\n",
    "    for idx in indices_diff_1: \n",
    "        merged_log[episodes[idx]+1] = 1 \n",
    "    for idx_2 in indices_diff_2:\n",
    "        merged_log[episodes[idx_2]+1] = 1\n",
    "        merged_log[episodes[idx_2]+2] = 1\n",
    "    return merged_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_files = get_csv_files(study_directory)\n",
    "dataframes = [pd.read_csv(file) for file in subject_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "fog_df = pd.DataFrame(columns = [\"filename\", \"NN\", \"MM\", \"very short\", \n",
    "                                 \"short\", \"long\", \"very long\", \"total\"])\n",
    "for df in dataframes:\n",
    "    filename = os.path.basename(os.path.normpath(subject_files[x]))\n",
    "    \n",
    "    r_acc_ap = df[r_acc_ap_col].values\n",
    "    l_acc_ap = df[l_acc_ap_col].values\n",
    "    r_gyr_ml = df[r_gyr_ml_col].values\n",
    "    l_gyr_ml = df[l_gyr_ml_col].values\n",
    "    \n",
    "    #Resample the data to 200 \n",
    "    r_ml = rs.resample(r_gyr_ml, sample_rate, step)\n",
    "    l_ml = rs.resample(l_gyr_ml, sample_rate, step)\n",
    "    r_ap = rs.resample(r_acc_ap, sample_rate, step)\n",
    "    l_ap = rs.resample(l_acc_ap, sample_rate, step)\n",
    "    \n",
    "    #apply low-pass butterworth filter\n",
    "    b,a = sp.signal.butter(4,5/(200/2))\n",
    "    sensor_data_filtered=sp.signal.filtfilt(b,a,[r_ml, l_ml], padtype = 'odd', padlen=3*(max(len(b),len(a))-1));\n",
    "    \n",
    "    #correlation-based method\n",
    "    starter_idx = get_starter_idx([r_ml, l_ml])\n",
    "    starter = round(abs(starter_idx))\n",
    "    corr_log = correlation_based_method(starter, sensor_data_filtered)\n",
    "\n",
    "    #fft-based method\n",
    "    fft_log = fft_method(starter, [r_ap, l_ap], len(sensor_data_filtered[0]))\n",
    "    \n",
    "    #Find places where logs agree\n",
    "    final_log = find_log_overlap(corr_log, fft_log)\n",
    "    \n",
    "    #Merge FOGS with one or two second apart\n",
    "    merged_log = merge_one_two_second(final_log)\n",
    "    \n",
    "    nn = merged_log.count(1)\n",
    "    mm = len(merged_log)\n",
    "    \n",
    "    #Find elements where values changed\n",
    "    nonzero_idxs = [i for i, x in enumerate(np.diff(merged_log)) if x != 0]\n",
    "    \n",
    "    #Find elements with minimal repetition\n",
    "    logical_arr = [None] * len(nonzero_idxs)\n",
    "    for idx, val in enumerate(nonzero_idxs):\n",
    "        logical_arr[idx] = merged_log[val]\n",
    "    dist_between_idxs = np.diff(nonzero_idxs)\n",
    "    \n",
    "    very_short = 0\n",
    "    short = 0\n",
    "    long = 0\n",
    "    very_long = 0\n",
    "    \n",
    "    #record fogs\n",
    "    for yes, idx in zip(logical_arr, dist_between_idxs.tolist()):\n",
    "        if yes==1 and idx == 1:\n",
    "            very_short = very_short + 1\n",
    "        elif yes==1 and idx>=2 and idx<=5:\n",
    "            short = short + 1\n",
    "        elif yes==1 and idx > 5 and idx<=30:\n",
    "            long = long + 1\n",
    "        elif yes==1 and idx > 30:\n",
    "            very_long = very_long + 1\n",
    "    \n",
    "    total = 100*((fog.get_nn() - fog.get_very_short())/fog.get_mm())\n",
    "    \n",
    "    fog_df = fog_df.append({'filename': filename,\n",
    "                              'NN': nn,\n",
    "                              'MM': mm,\n",
    "                             'very short': very_short,\n",
    "                             'short': short,\n",
    "                             'long': long,\n",
    "                             'very long': very_long,\n",
    "                             'total': total}, ignore_index = True)\n",
    "    x = x + 1\n",
    "fog_df.to_excel(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
